{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "workspace_dir = \"/workspace/neelnanda\"\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/.cache\"\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model1_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model1_name,torch_dtype=\"auto\").to(device)\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model1_name, padding_side=\"left\")\n",
    "\n",
    "model0_name = \"Qwen/Qwen2.5-Math-1.5B\"\n",
    "model0 = AutoModelForCausalLM.from_pretrained(model0_name,torch_dtype=\"auto\").to(device)\n",
    "tokenizer0 = AutoTokenizer.from_pretrained(model0_name, padding_side=\"left\")\n",
    "\n",
    "def hf_call_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sys_prompts: List[str],\n",
    "    user_prompts: List[str],\n",
    "    generation_prompt: str = \"<｜Assistant｜>\",\n",
    "    batch_size: int = 200,\n",
    "    max_length: int = 1000,\n",
    "    temp: float = 0.7,\n",
    "):\n",
    "    assert len(sys_prompts) == len(user_prompts)\n",
    "    responses = []\n",
    "\n",
    "    for i in range(0, len(sys_prompts), batch_size):\n",
    "        # print(f\"Starting batch {i}\")\n",
    "\n",
    "        end = min(i + batch_size, len(sys_prompts))\n",
    "        batch_sys_prompts = sys_prompts[i:end]\n",
    "        batch_user_prompts = user_prompts[i:end]\n",
    "\n",
    "        batch_messages = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "            for sys_prompt, user_prompt in zip(\n",
    "                batch_sys_prompts, batch_user_prompts\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        batch_texts = [\n",
    "            tokenizer1.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            for messages in batch_messages\n",
    "        ]\n",
    "\n",
    "        print(batch_texts[0])\n",
    "\n",
    "        # Tokenize all prompts in batch\n",
    "        model_inputs = tokenizer(\n",
    "            batch_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate responses autoregressively\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            attention_mask=model_inputs.attention_mask,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temp,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        # Extract only the generated tokens (excluding input prompt tokens)\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        # Decode generated tokens to text\n",
    "        batch_responses = tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        responses.extend(batch_responses)\n",
    "\n",
    "        # print(f\"Batch {i} took {time.time()-start_time} seconds\")\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>What is 1+1? <｜Assistant｜><think>\n",
      "\n",
      "[\"Okay, so I need to figure out what 1 plus 1 is. Hmm, that seems pretty straightforward, but I guess I should think about it carefully to make sure I'm not missing anything. Let me start by recalling what I know about addition. Addition is combining two numbers to get a total. So, in this case, I have two numbers: 1 and 1.\\n\\nI know that 1 is a unit, representing a single object. When I add another 1, I'm essentially combining another single object with the first one. So, if I have one apple and then get another apple, I now have two apples. That makes sense.\\n\\nBut wait, maybe I should consider different contexts. For example, what if I'm counting on my fingers? If I have one finger up and then put another finger up, I can see that there are two fingers up. That's another way to visualize addition.\\n\\nI also remember that in mathematics, addition is commutative, which means the order of the numbers doesn't matter. So, 1 + 1 is the same as 1 + 1, and both equal 2. That's good to know because it confirms that the result is consistent regardless of the order.\\n\\nIs there any other way to think about this? Maybe using objects. If I take two blocks and put them together, they form a pair. So, that's another example of adding two single units to make a new unit, which is two.\\n\\nI should also consider if there are any exceptions or special cases. For instance, what if I add something negative? Like 1 + (-1). That would be zero. But in this case, both numbers are positive, so it's just 1 + 1 = 2.\\n\\nAnother angle could be using number lines. Starting at 1 and moving one step to the right lands me at 2, which is the sum of 1 and 1.\\n\\nI guess there's also a concept in programming where 1 + 1 is equal to 10 in binary, but that's a different context. Here, since we're dealing with basic arithmetic, it's just 2.\\n\\nSo, putting it all together, no matter how I think about it—visually, numerically, object-based, or even in different mathematical contexts—it consistently results in 2.\\n</think>\\n\\nThe sum of 1 and 1 is 2. This can be understood through various contexts, such as combining objects, using fingers, counting on fingers, or mathematical principles like commutativity. Therefore, 1 + 1 equals 2.\\n\\n**Answer:** 2\"]\n"
     ]
    }
   ],
   "source": [
    "print(hf_call_batch(\n",
    "    model=model1,\n",
    "    tokenizer=tokenizer1,\n",
    "    sys_prompts=[\"You are a helpful assistant.\"], \n",
    "    user_prompts=[\"\"\"What is 1+1? \"\"\"],\n",
    "    max_length=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543714304"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model1.model.layers[0].mlp.down_proj.parameters():\n",
    "    k1 = m\n",
    "\n",
    "for m in model0.model.layers[0].mlp.down_proj.parameters():\n",
    "    k0 = m\n",
    "\n",
    "k_diff = k1-k0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0045, -0.0094,  0.0013,  ..., -0.0024,  0.0108, -0.0087],\n",
       "        [-0.0068, -0.0045,  0.0026,  ...,  0.0046,  0.0015,  0.0040],\n",
       "        [-0.0031,  0.0060,  0.0005,  ..., -0.0022,  0.0020,  0.0056],\n",
       "        ...,\n",
       "        [-0.0118,  0.0061,  0.0002,  ..., -0.0060, -0.0023,  0.0021],\n",
       "        [-0.0054,  0.0090,  0.0027,  ..., -0.0002,  0.0015, -0.0001],\n",
       "        [ 0.0022,  0.0005,  0.0039,  ...,  0.0032,  0.0010,  0.0014]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model0 = HookedTransformer.from_pretrained(\"Qwen/Qwen2.5-1.5B\", device=device)\n",
    "model1 = HookedTransformer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", device=device)\n",
    "tokenizer = model0.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tokenizers (like GPT-2’s) do not have a pad token. In that case, use the EOS token.\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "########################################################################\n",
    "# Define a dummy text classification dataset.\n",
    "########################################################################\n",
    "\n",
    "class DummyTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Tokenize the text (using the model's tokenizer)\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        return tokens, label\n",
    "\n",
    "# Example data (modify with your own dataset)\n",
    "texts = [\n",
    "    \"This is a positive example.\",\n",
    "    \"This is a negative example.\",\n",
    "    \"Yet another positive text.\",\n",
    "    \"And one more negative instance.\"\n",
    "]\n",
    "# For our dummy task, we use 1 for positive and 0 for negative\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "dataset = DummyTextDataset(texts, labels, tokenizer)\n",
    "# Define a simple collate function that pads the token sequences\n",
    "def collate_fn(batch):\n",
    "    tokens, labels = zip(*batch)\n",
    "    tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "        tokens, batch_first=True, padding_value=pad_token_id\n",
    "    )\n",
    "    labels = torch.tensor(labels)\n",
    "    return tokens, labels\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "########################################################################\n",
    "# Define a function to extract the last-layer activation.\n",
    "#\n",
    "# In this example we hook the last block’s “resid_post” (i.e. the output\n",
    "# of the block, before the final layer norm and head) and then select the\n",
    "# representation corresponding to the final (non-pad) token in each sequence.\n",
    "########################################################################\n",
    "\n",
    "def get_last_layer_activation(tokens):\n",
    "    # Dictionary to store the activation from our hook.\n",
    "    activation = {}\n",
    "\n",
    "    def hook_fn(res, hook):\n",
    "        # Save a detached copy of the activation.\n",
    "        activation[\"last_layer\"] = res.detach()\n",
    "    \n",
    "    # Choose the last block (0-indexed) – note: model.cfg.n_layers is the total count.\n",
    "    last_layer = model0.cfg.n_layers - 1\n",
    "    # The hook name for the final block’s output (residual stream after the block).\n",
    "    hook_name = f\"blocks.{last_layer}.hook_resid_post\"\n",
    "    \n",
    "    # Run the model with our hook.\n",
    "    # Here we use `run_with_hooks` so that the hook function is called during the forward pass.\n",
    "    _ = model0.run_with_hooks(tokens, hook_dict={hook_name: hook_fn}, return_type=\"both\")\n",
    "    \n",
    "    # activation[\"last_layer\"] has shape (batch, seq_len, d_model)\n",
    "    # For this probe, we select the last non-pad token from each sequence.\n",
    "    # Compute the length (number of non-pad tokens) per example.\n",
    "    lengths = tokens.ne(pad_token_id).sum(dim=1)  # shape: (batch,)\n",
    "    # Index: for each example i, select position lengths[i]-1\n",
    "    last_token_rep = activation[\"last_layer\"][torch.arange(activation[\"last_layer\"].shape[0]), lengths - 1]\n",
    "    \n",
    "    return last_token_rep  # shape: (batch, d_model)\n",
    "\n",
    "########################################################################\n",
    "# Define the linear probe model.\n",
    "#\n",
    "# This is a simple linear classifier that maps from the transformer's \n",
    "# hidden dimension to the number of classes.\n",
    "########################################################################\n",
    "\n",
    "num_classes = 2\n",
    "d_model = model0.cfg.d_model  # hidden dimension of the transformer\n",
    "probe = nn.Linear(d_model, num_classes)\n",
    "\n",
    "# Set up an optimizer and a loss function (cross-entropy for classification).\n",
    "probe_optimizer = optim.Adam(probe.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "########################################################################\n",
    "# Training loop for the linear probe.\n",
    "########################################################################\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for tokens, labels in dataloader:\n",
    "        probe_optimizer.zero_grad()\n",
    "        # Get the last-layer representations from the model\n",
    "        reps = get_last_layer_activation(tokens)  # shape: (batch, d_model)\n",
    "        # Forward pass through the probe\n",
    "        logits = probe(reps)  # shape: (batch, num_classes)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        probe_optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} -- Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 1. Load the pretrained model\n",
    "# ---------------------------\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "tokenizer = model.tokenizer\n",
    "# Some tokenizers (like GPT-2’s) do not have a pad token, so we use the EOS token.\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "# Set model to train mode so gradients are computed.\n",
    "model.train()\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define a linear projection to output a scalar.\n",
    "# ---------------------------\n",
    "d_model = model.cfg.d_model  # hidden dimension of the transformer\n",
    "projection = nn.Linear(d_model, 1)  # maps from d_model to a single real number\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Prepare a single datapoint.\n",
    "# ---------------------------\n",
    "text = \"This is an example datapoint.\"\n",
    "# Tokenize the text. The output has shape (1, seq_len).\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Extract the last-layer residual stream via a hook.\n",
    "# ---------------------------\n",
    "# We'll store the activation in a dictionary.\n",
    "activation = {}\n",
    "\n",
    "def hook_fn(res, hook):\n",
    "    # Save the activation (we don't detach here because we want gradients to flow)\n",
    "    activation[\"last_layer\"] = res\n",
    "\n",
    "# Identify the last transformer block (0-indexed).\n",
    "last_layer_idx = model.cfg.n_layers - 1\n",
    "hook_name = f\"blocks.{last_layer_idx}.hook_resid_post\"\n",
    "\n",
    "# Run the model with the hook attached.\n",
    "# We use run_with_hooks so that the hook_fn is executed during the forward pass.\n",
    "# We set return_type=\"none\" because we don't need the model's final output.\n",
    "_ = model.run_with_hooks(tokens, hook_dict={hook_name: hook_fn}, return_type=\"none\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Extract the representation for the last non-pad token.\n",
    "# ---------------------------\n",
    "# activation[\"last_layer\"] has shape (batch, seq_len, d_model)\n",
    "# Compute the number of non-pad tokens in each sequence.\n",
    "lengths = tokens.ne(pad_token_id).sum(dim=1)  # shape: (batch,)\n",
    "# For each example, pick the representation of the last non-pad token.\n",
    "last_token_rep = activation[\"last_layer\"][torch.arange(activation[\"last_layer\"].shape[0]), lengths - 1]\n",
    "# last_token_rep now has shape (batch, d_model)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Apply the linear projection to get a scalar output.\n",
    "# ---------------------------\n",
    "# The projection outputs shape (batch, 1). We squeeze it to get a scalar per datapoint.\n",
    "scalar_output = projection(last_token_rep).squeeze()\n",
    "# If batch size is 1, scalar_output is a single scalar.\n",
    "\n",
    "print(\"Scalar output:\", scalar_output.item())\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Backward pass: compute gradients w.r.t. model and projection parameters.\n",
    "# ---------------------------\n",
    "# Compute gradients of the scalar output with respect to all parameters.\n",
    "scalar_output.backward()\n",
    "\n",
    "# Cache the gradients for the model's parameters.\n",
    "model_grads = {\n",
    "    name: param.grad.clone() \n",
    "    for name, param in model.named_parameters() \n",
    "    if param.grad is not None\n",
    "}\n",
    "\n",
    "# Similarly, cache the gradients for the projection's parameters.\n",
    "projection_grads = {\n",
    "    name: param.grad.clone()\n",
    "    for name, param in projection.named_parameters()\n",
    "    if param.grad is not None\n",
    "}\n",
    " \n",
    "# ---------------------------\n",
    "# 8. (Optional) Inspect the gradient norms.\n",
    "# ---------------------------\n",
    "print(\"Model gradients (norms):\")\n",
    "for name, grad in model_grads.items():\n",
    "    print(f\"  {name}: {grad.norm().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: tensor(3.2925, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_description_text = \"\"\"## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "loss = model0(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
